apiVersion: apps/v1
kind: Deployment
metadata:
  name: drift-monitor
  labels:
    app: drift-monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: drift-monitor
  template:
    metadata:
      labels:
        app: drift-monitor
    spec:
      containers:
      - name: monitor
        image: python:3.9-slim
        command: ["python", "-u", "-c"]
        args:
        - |
          import time
          import os
          import sys
          
          ACCURACY_THRESHOLD = 0.8
          
          def check_models():
              model_name = os.environ.get('MODEL_NAME', 'test-model-v1.pkl')
              print(f"Checking model: {model_name}", flush=True)
              
              # Simulate accuracy check (matches simulate-drift.py output)
              simulated_accuracy = 0.62
              
              if simulated_accuracy < ACCURACY_THRESHOLD:
                  print(f"ALERT: Model {model_name} accuracy {simulated_accuracy} below threshold {ACCURACY_THRESHOLD}", flush=True)
                  print("ALERT: Drift detected! Model performance has degraded.", flush=True)
                  print("ALERT: Recommending rollback to previous version.", flush=True)
              else:
                  print(f"OK: Model {model_name} accuracy {simulated_accuracy}", flush=True)
          
          print("Drift monitor started...", flush=True)
          print("Monitoring for model drift every 30 seconds...", flush=True)
          sys.stdout.flush()
          while True:
              check_models()
              sys.stdout.flush()
              time.sleep(30)
        env:
        - name: MODEL_NAME
          value: "test-model-v1.pkl"
        - name: PYTHONUNBUFFERED
          value: "1"
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
